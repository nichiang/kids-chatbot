#!/usr/bin/env python3
"""
Latency Analysis Tool for English Learning Chatbot

This tool analyzes latency logs generated by the latency measurement system
to provide insights into application performance and identify optimization opportunities.

Usage:
    python analyze_latency.py
    python analyze_latency.py --detailed
    python analyze_latency.py --story-only
"""

import json
import statistics
import argparse
from datetime import datetime, timedelta
from pathlib import Path
import os

class LatencyAnalyzer:
    def __init__(self, logs_dir="logs"):
        self.logs_dir = Path(logs_dir)
        self.request_data = []
        self.story_data = []
        
    def load_data(self):
        """Load latency data from JSON log files"""
        # Load request latency data
        request_log = self.logs_dir / "latency.jsonl"
        if request_log.exists():
            with open(request_log, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            self.request_data.append(json.loads(line))
                        except json.JSONDecodeError:
                            continue
        
        # Load story completion data
        story_log = self.logs_dir / "story_latency.jsonl"
        if story_log.exists():
            with open(story_log, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            self.story_data.append(json.loads(line))
                        except json.JSONDecodeError:
                            continue
    
    def analyze_requests(self):
        """Analyze general request latency data"""
        if not self.request_data:
            print("‚ùå No request latency data found")
            return
        
        total_times = [req['total_request_time'] for req in self.request_data]
        llm_times = [req['llm_total_time'] for req in self.request_data]
        processing_times = [req['processing_time'] for req in self.request_data]
        
        print("\n" + "=" * 50)
        print("üìä REQUEST LATENCY ANALYSIS")
        print("=" * 50)
        print(f"üìà Total Requests Analyzed: {len(self.request_data)}")
        print(f"‚è±Ô∏è  Average Total Time: {statistics.mean(total_times):.1f}ms")
        print(f"üìä Median Total Time: {statistics.median(total_times):.1f}ms")
        print(f"üìà 95th Percentile: {self._percentile(total_times, 95):.1f}ms")
        print(f"‚ö° Fastest Request: {min(total_times):.1f}ms")
        print(f"üêå Slowest Request: {max(total_times):.1f}ms")
        
        print(f"\nü§ñ LLM Performance:")
        print(f"   Average LLM Time: {statistics.mean(llm_times):.1f}ms")
        print(f"   LLM vs Total: {statistics.mean(llm_times)/statistics.mean(total_times)*100:.1f}%")
        
        print(f"\n‚öôÔ∏è  Processing Performance:")
        print(f"   Average Processing Time: {statistics.mean(processing_times):.1f}ms")
        print(f"   Processing vs Total: {statistics.mean(processing_times)/statistics.mean(total_times)*100:.1f}%")
        
        # Analyze by LLM call count
        self._analyze_by_llm_calls()
        
        # Analyze by result type
        self._analyze_by_result_type()
    
    def analyze_stories(self):
        """Analyze story-specific latency data"""
        if not self.story_data:
            print("‚ùå No story latency data found")
            return
        
        print("\n" + "=" * 50)
        print("üìö STORY LATENCY ANALYSIS")
        print("=" * 50)
        
        story_averages = [story['average_latency'] for story in self.story_data]
        exchange_counts = [story['total_exchanges'] for story in self.story_data]
        story_durations = [story['total_story_duration'] for story in self.story_data]
        
        print(f"üìñ Completed Stories: {len(self.story_data)}")
        print(f"‚è±Ô∏è  Average Story Exchange Latency: {statistics.mean(story_averages):.1f}ms")
        print(f"üìä Median Story Exchange Latency: {statistics.median(story_averages):.1f}ms")
        print(f"üìà 95th Percentile Exchange Latency: {self._percentile(story_averages, 95):.1f}ms")
        
        print(f"\nüìù Story Structure:")
        print(f"   Average Exchanges per Story: {statistics.mean(exchange_counts):.1f}")
        print(f"   Average Story Duration: {statistics.mean(story_durations):.1f} seconds")
        
        # Analyze by topic
        self._analyze_by_topic()
        
        # Analyze exchange types
        self._analyze_exchange_types()
    
    def _analyze_by_llm_calls(self):
        """Analyze latency by number of LLM calls"""
        by_calls = {}
        for req in self.request_data:
            count = req['llm_call_count']
            if count not in by_calls:
                by_calls[count] = []
            by_calls[count].append(req['total_request_time'])
        
        print(f"\nüî¢ Performance by LLM Call Count:")
        for count in sorted(by_calls.keys()):
            times = by_calls[count]
            print(f"   {count} calls: {statistics.mean(times):.1f}ms avg ({len(times)} requests)")
    
    def _analyze_by_result_type(self):
        """Analyze latency by response type"""
        by_type = {}
        for req in self.request_data:
            result_type = req.get('result_type', 'unknown')
            if result_type not in by_type:
                by_type[result_type] = []
            by_type[result_type].append(req['total_request_time'])
        
        print(f"\nüìã Performance by Response Type:")
        for type_name in sorted(by_type.keys()):
            times = by_type[type_name]
            print(f"   {type_name}: {statistics.mean(times):.1f}ms avg ({len(times)} responses)")
    
    def _analyze_by_topic(self):
        """Analyze story latency by topic"""
        by_topic = {}
        for story in self.story_data:
            topic = story.get('topic', 'unknown')
            if topic not in by_topic:
                by_topic[topic] = []
            by_topic[topic].append(story['average_latency'])
        
        print(f"\nüåü Performance by Story Topic:")
        for topic in sorted(by_topic.keys()):
            latencies = by_topic[topic]
            print(f"   {topic}: {statistics.mean(latencies):.1f}ms avg ({len(latencies)} stories)")
    
    def _analyze_exchange_types(self):
        """Analyze latency by exchange type within stories"""
        exchange_types = {}
        
        for story in self.story_data:
            for exchange in story.get('exchanges', []):
                ex_type = exchange['exchange_type']
                if ex_type not in exchange_types:
                    exchange_types[ex_type] = []
                exchange_types[ex_type].append(exchange['latency'])
        
        if exchange_types:
            print(f"\nüîÑ Performance by Exchange Type:")
            for ex_type in sorted(exchange_types.keys()):
                latencies = exchange_types[ex_type]
                print(f"   {ex_type}: {statistics.mean(latencies):.1f}ms avg ({len(latencies)} exchanges)")
    
    def detailed_analysis(self):
        """Provide detailed analysis with specific insights"""
        print("\n" + "=" * 50)
        print("üîç DETAILED PERFORMANCE INSIGHTS")
        print("=" * 50)
        
        if self.request_data:
            # Identify slowest requests
            slowest = sorted(self.request_data, key=lambda x: x['total_request_time'], reverse=True)[:5]
            print(f"\nüêå Top 5 Slowest Requests:")
            for i, req in enumerate(slowest, 1):
                print(f"   {i}. {req['total_request_time']:.1f}ms ({req['llm_call_count']} LLM calls, {req.get('result_type', 'unknown')})")
            
            # LLM vs Processing time analysis
            llm_times = [req['llm_total_time'] for req in self.request_data]
            processing_times = [req['processing_time'] for req in self.request_data]
            
            print(f"\n‚öñÔ∏è  LLM vs Processing Time Analysis:")
            print(f"   LLM Time Distribution:")
            print(f"     Min: {min(llm_times):.1f}ms, Max: {max(llm_times):.1f}ms")
            print(f"   Processing Time Distribution:")
            print(f"     Min: {min(processing_times):.1f}ms, Max: {max(processing_times):.1f}ms")
            
            # Identify performance bottlenecks
            avg_llm = statistics.mean(llm_times)
            avg_processing = statistics.mean(processing_times)
            
            if avg_llm > avg_processing * 2:
                print(f"   üö® LLM calls are the primary bottleneck ({avg_llm:.1f}ms vs {avg_processing:.1f}ms)")
            elif avg_processing > avg_llm * 2:
                print(f"   üö® Processing time is the primary bottleneck ({avg_processing:.1f}ms vs {avg_llm:.1f}ms)")
            else:
                print(f"   ‚úÖ Balanced performance between LLM and processing")
        
        if self.story_data:
            print(f"\nüìà Story Performance Trends:")
            
            # Analyze if latency increases with exchange count
            exchange_vs_latency = [(story['total_exchanges'], story['average_latency']) for story in self.story_data]
            if len(exchange_vs_latency) > 1:
                exchanges = [x[0] for x in exchange_vs_latency]
                latencies = [x[1] for x in exchange_vs_latency]
                correlation = self._simple_correlation(exchanges, latencies)
                
                if correlation > 0.3:
                    print(f"   ‚ö†Ô∏è  Latency increases with story length (correlation: {correlation:.2f})")
                elif correlation < -0.3:
                    print(f"   ‚úÖ Latency decreases with story length (correlation: {correlation:.2f})")
                else:
                    print(f"   ‚úÖ Story length doesn't significantly affect latency")
    
    def performance_recommendations(self):
        """Provide performance optimization recommendations"""
        print("\n" + "=" * 50)
        print("üí° PERFORMANCE RECOMMENDATIONS")
        print("=" * 50)
        
        if not self.request_data:
            print("‚ùå No data available for recommendations")
            return
        
        avg_total = statistics.mean([req['total_request_time'] for req in self.request_data])
        avg_llm = statistics.mean([req['llm_total_time'] for req in self.request_data])
        
        print(f"üìã Current Performance Status:")
        print(f"   Average Response Time: {avg_total:.1f}ms")
        
        # Educational context recommendations
        if avg_total > 3000:  # Over 3 seconds
            print(f"   üö® CRITICAL: Response time exceeds child attention span threshold")
            print(f"   üéØ Target: Under 2000ms for educational engagement")
        elif avg_total > 2000:  # Over 2 seconds
            print(f"   ‚ö†Ô∏è  WARNING: Response time approaches attention span limits")
            print(f"   üéØ Target: Under 1500ms for optimal child experience")
        else:
            print(f"   ‚úÖ Response time is within acceptable range for children")
        
        print(f"\nüîß Optimization Opportunities:")
        
        # LLM optimization
        llm_percentage = (avg_llm / avg_total) * 100
        if llm_percentage > 70:
            print(f"   ü§ñ LLM Optimization (High Priority):")
            print(f"      - Consider parallel LLM calls where possible")
            print(f"      - Implement response caching for common requests")
            print(f"      - Use shorter prompts for non-critical calls")
        
        # Processing optimization
        processing_percentage = 100 - llm_percentage
        if processing_percentage > 30:
            print(f"   ‚öôÔ∏è  Processing Optimization:")
            print(f"      - Profile vocabulary selection algorithms")
            print(f"      - Optimize session data management")
            print(f"      - Consider async processing for non-blocking operations")
        
        # Story-specific recommendations
        if self.story_data:
            avg_story_latency = statistics.mean([story['average_latency'] for story in self.story_data])
            if avg_story_latency > avg_total:
                print(f"   üìö Story Mode Optimization:")
                print(f"      - Story exchanges are slower than average")
                print(f"      - Consider optimizing character/location design prompts")
                print(f"      - Review vocabulary integration complexity")
        
        print(f"\nüéØ Solution 1 Impact Assessment:")
        print(f"   Current avg response time: {avg_total:.1f}ms")
        print(f"   Solution 1 overhead: +200-500ms (intent classification)")
        print(f"   Projected total time: {avg_total + 350:.1f}ms")
        
        if avg_total + 350 > 2000:
            print(f"   ‚ö†Ô∏è  Solution 1 would push response time over child-friendly threshold")
            print(f"   üìã Recommend optimizing current performance before implementing")
        else:
            print(f"   ‚úÖ Solution 1 implementation appears feasible")
    
    def _percentile(self, data, percentile):
        """Calculate percentile of data"""
        if not data:
            return 0
        sorted_data = sorted(data)
        index = int((percentile / 100) * len(sorted_data))
        return sorted_data[min(index, len(sorted_data) - 1)]
    
    def _simple_correlation(self, x, y):
        """Calculate simple correlation coefficient"""
        if len(x) != len(y) or len(x) < 2:
            return 0
        
        mean_x = statistics.mean(x)
        mean_y = statistics.mean(y)
        
        numerator = sum((x[i] - mean_x) * (y[i] - mean_y) for i in range(len(x)))
        sum_sq_x = sum((x[i] - mean_x) ** 2 for i in range(len(x)))
        sum_sq_y = sum((y[i] - mean_y) ** 2 for i in range(len(y)))
        
        denominator = (sum_sq_x * sum_sq_y) ** 0.5
        
        if denominator == 0:
            return 0
        
        return numerator / denominator

def main():
    parser = argparse.ArgumentParser(description='Analyze latency logs for educational chatbot')
    parser.add_argument('--detailed', action='store_true', help='Show detailed analysis')
    parser.add_argument('--story-only', action='store_true', help='Analyze only story data')
    parser.add_argument('--logs-dir', default='logs', help='Directory containing log files')
    
    args = parser.parse_args()
    
    analyzer = LatencyAnalyzer(args.logs_dir)
    analyzer.load_data()
    
    if not analyzer.request_data and not analyzer.story_data:
        print("No latency data found!")
        print(f"   Checked directory: {os.path.abspath(args.logs_dir)}")
        print("   Make sure the application has generated some latency logs first.")
        return
    
    print("ENGLISH LEARNING CHATBOT - LATENCY ANALYSIS")
    print(f"Analyzing logs from: {os.path.abspath(args.logs_dir)}")
    
    if not args.story_only:
        analyzer.analyze_requests()
    
    analyzer.analyze_stories()
    
    if args.detailed:
        analyzer.detailed_analysis()
    
    analyzer.performance_recommendations()

if __name__ == "__main__":
    main()